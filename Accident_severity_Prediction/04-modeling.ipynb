{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc0dfd5",
   "metadata": {},
   "source": [
    "# Road Accident Severity Prediction\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b41d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. PREP DATA FOR MODELLING\n",
    "#     - select feature columns that EDA showed important\n",
    "# ============================================================\n",
    "\n",
    "feature_cols = [\n",
    "    # accident numeric\n",
    "    \"Number_of_Vehicles\",\n",
    "    \"Number_of_Casualties\",\n",
    "    \"Speed_limit\",\n",
    "    \"Hour\",\n",
    "    \"Month\",\n",
    "    \"Is_Weekend\",\n",
    "    \"veh_count\",\n",
    "    \"veh_age_mean\",\n",
    "    \"engine_mean\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    # accident categorical\n",
    "    \"Weather_Conditions\",\n",
    "    \"Light_Conditions\",\n",
    "    \"Road_Type\",\n",
    "    \"Road_Surface_Conditions\",\n",
    "    \"Urban_or_Rural_Area\",\n",
    "    \"Junction_Detail\",\n",
    "    \"Junction_Control\",\n",
    "    \"Carriageway_Hazards\",\n",
    "    \"Special_Conditions_at_Site\",\n",
    "]\n",
    "\n",
    "# keep only columns that actually exist\n",
    "feature_cols = [c for c in feature_cols if c in acc.columns]\n",
    "\n",
    "data = acc[feature_cols + [target_col]].copy()\n",
    "print(\"Total rows before dropping NA:\", data.shape)\n",
    "data = data.dropna(subset=[target_col])\n",
    "print(\"Rows after dropping rows with missing target:\", data.shape)\n",
    "\n",
    "X_full = data[feature_cols]\n",
    "y_full = data[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e23c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. HANDLE MISSING VALUES & CATEGORICAL TYPES\n",
    "# ============================================================\n",
    "\n",
    "# identify categorical & numeric features\n",
    "cat_features = X_full.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_features = [c for c in feature_cols if c not in cat_features]\n",
    "\n",
    "# fill numerics with median, categoricals with 'Unknown'\n",
    "for col in num_features:\n",
    "    X_full[col] = pd.to_numeric(X_full[col], errors=\"coerce\")\n",
    "    X_full[col] = X_full[col].fillna(X_full[col].median())\n",
    "\n",
    "for col in cat_features:\n",
    "    X_full[col] = X_full[col].astype(\"category\")\n",
    "    # Only add \"Unknown\" if it's not already in categories\n",
    "    if \"Unknown\" not in X_full[col].cat.categories:\n",
    "        X_full[col] = X_full[col].cat.add_categories([\"Unknown\"])\n",
    "    X_full[col] = X_full[col].fillna(\"Unknown\")\n",
    "\n",
    "X_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbaeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11. STRATIFIED SAMPLE (300k) TO TRAIN ON LAPTOP\n",
    "# ============================================================\n",
    "\n",
    "N = 300_000\n",
    "if len(X_full) > N:\n",
    "    X_sample, _, y_sample, _ = train_test_split(\n",
    "        X_full, y_full,\n",
    "        train_size=N,\n",
    "        stratify=y_full,\n",
    "        random_state=42\n",
    "    )\n",
    "else:\n",
    "    X_sample, y_sample = X_full, y_full\n",
    "\n",
    "print(\"Sampled data shape:\", X_sample.shape)\n",
    "print(\"Sampled severity distribution:\")\n",
    "print(y_sample.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8afa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12. TRAIN/TEST SPLIT\n",
    "# ============================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample,\n",
    "    y_sample,\n",
    "    test_size=0.2,\n",
    "    stratify=y_sample,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a763dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 13. ENCODING FOR MODELS\n",
    "#     - RF/XGB: OrdinalEncoder\n",
    "#     - LGBM/CatBoost: use categories directly\n",
    "# ============================================================\n",
    "\n",
    "# 13.1 Ordinal encoding for RF and XGB\n",
    "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "X_train_enc = X_train.copy()\n",
    "X_test_enc  = X_test.copy()\n",
    "\n",
    "if cat_features:\n",
    "    X_train_enc[cat_features] = encoder.fit_transform(X_train[cat_features])\n",
    "    X_test_enc[cat_features]  = encoder.transform(X_test[cat_features])\n",
    "\n",
    "X_train_enc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f9896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.2 Category dtype for LightGBM & CatBoost\n",
    "X_train_lgb = X_train.copy()\n",
    "X_test_lgb  = X_test.copy()\n",
    "for col in cat_features:\n",
    "    X_train_lgb[col] = X_train_lgb[col].astype(\"category\")\n",
    "    X_test_lgb[col]  = X_test_lgb[col].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d95cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 14. CLASS WEIGHTS FOR IMBALANCE\n",
    "# ============================================================\n",
    "\n",
    "class_counts = y_train.value_counts().to_dict()\n",
    "num_classes = len(class_counts)\n",
    "total = len(y_train)\n",
    "class_weights = {cls: total / (num_classes * cnt)\n",
    "                 for cls, cnt in class_counts.items()}\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 15. TRAIN MODELS (RF, XGB, LGBM, CatBoost)\n",
    "# ============================================================\n",
    "\n",
    "results = {}\n",
    "all_preds = {}\n",
    "all_proba = {}\n",
    "\n",
    "# 15.1 Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    class_weight=class_weights,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_enc, y_train)\n",
    "y_pred_rf = rf.predict(X_test_enc)\n",
    "y_proba_rf = rf.predict_proba(X_test_enc)\n",
    "\n",
    "all_preds[\"RandomForest\"] = y_pred_rf\n",
    "all_proba[\"RandomForest\"] = y_proba_rf\n",
    "results[\"RandomForest\"] = f1_score(y_test, y_pred_rf, average=\"macro\")\n",
    "print(\"Random Forest Macro F1:\", results[\"RandomForest\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e65055",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ac770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15.2 XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=3,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,\n",
    "    n_estimators=300,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_enc, y_train)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test_enc)\n",
    "y_pred_xgb  = np.argmax(y_proba_xgb, axis=1)\n",
    "\n",
    "all_preds[\"XGBoost\"] = y_pred_xgb\n",
    "all_proba[\"XGBoost\"] = y_proba_xgb\n",
    "results[\"XGBoost\"] = f1_score(y_test, y_pred_xgb, average=\"macro\")\n",
    "print(\"XGBoost Macro F1:\", results[\"XGBoost\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276430d",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e34375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15.3 LightGBM\n",
    "sample_weights = y_train.map(class_weights).values\n",
    "\n",
    "lgb_train = lgb.Dataset(\n",
    "    X_train_lgb,\n",
    "    label=y_train,\n",
    "    weight=sample_weights,\n",
    "    categorical_feature=cat_features or None\n",
    ")\n",
    "lgb_valid = lgb.Dataset(\n",
    "    X_test_lgb,\n",
    "    label=y_test,\n",
    "    categorical_feature=cat_features or None\n",
    ")\n",
    "\n",
    "params_lgb = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 3,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": -1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=50),\n",
    "    lgb.log_evaluation(period=100),\n",
    "]\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    params_lgb,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_proba_lgb = lgb_model.predict(X_test_lgb)\n",
    "y_pred_lgb  = np.argmax(y_proba_lgb, axis=1)\n",
    "\n",
    "all_preds[\"LightGBM\"] = y_pred_lgb\n",
    "all_proba[\"LightGBM\"] = y_proba_lgb\n",
    "results[\"LightGBM\"] = f1_score(y_test, y_pred_lgb, average=\"macro\")\n",
    "print(\"LightGBM Macro F1:\", results[\"LightGBM\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f04c7d",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82007944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15.4 CatBoost\n",
    "cat_indices = [X_train.columns.get_loc(c) for c in cat_features]\n",
    "\n",
    "cb_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"TotalF1\",\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cb_model.fit(X_train, y_train,\n",
    "             cat_features=cat_indices,\n",
    "             eval_set=(X_test, y_test),\n",
    "             verbose=False)\n",
    "\n",
    "y_proba_cb = cb_model.predict_proba(X_test)\n",
    "y_pred_cb  = cb_model.predict(X_test).astype(int).ravel()\n",
    "\n",
    "all_preds[\"CatBoost\"] = y_pred_cb\n",
    "all_proba[\"CatBoost\"] = y_proba_cb\n",
    "results[\"CatBoost\"] = f1_score(y_test, y_pred_cb, average=\"macro\")\n",
    "print(\"CatBoost Macro F1:\", results[\"CatBoost\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
